{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer for Document Understanding - Rapid Prototype Development\n",
    "// Discussion placeholder //\n",
    "\n",
    "// Outline //\n",
    "\n",
    "* Document Understanding Model Implementation\n",
    "* Data Preparation Pipeline\n",
    "* Multi-Lingual Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOCUMENT UNDERSTANDING MODEL IMPLEMENTATION\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "import numpy as np\n",
    "\n",
    "class DocumentExtractor(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DocumentExtractor, self).__init__()\n",
    "        \n",
    "        # Load pre-trained Donut model\n",
    "        self.processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "        self.base_model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "        \n",
    "        # Freeze base model layers\n",
    "        for layer in self.base_model.encoder.layers:\n",
    "            layer.trainable = False\n",
    "            \n",
    "        # Custom layers for specific field extraction\n",
    "        self.estimate_number_head = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(32)  # Output dimension for estimate number\n",
    "        ])\n",
    "        \n",
    "        self.address_head = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(64)  # Output dimension for address\n",
    "        ])\n",
    "        \n",
    "        self.total_amount_head = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(1)  # Single output for total amount\n",
    "        ])\n",
    "\n",
    "    def preprocess_image(self, image_path):\n",
    "        # Load and preprocess image using Donut processor\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.resize(image, [224, 224])\n",
    "        pixel_values = self.processor(image, return_tensors=\"tf\").pixel_values\n",
    "        return pixel_values\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Get base model features\n",
    "        features = self.base_model.encoder(inputs)\n",
    "        \n",
    "        # Extract specific fields using custom heads\n",
    "        estimate_number = self.estimate_number_head(features)\n",
    "        address = self.address_head(features)\n",
    "        total_amount = self.total_amount_head(features)\n",
    "        \n",
    "        return {\n",
    "            'estimate_number': estimate_number,\n",
    "            'address': address,\n",
    "            'total_amount': total_amount\n",
    "        }\n",
    "\n",
    "    def train_step(self, data):\n",
    "        images, labels = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            predictions = self(images, training=True)\n",
    "            \n",
    "            # Calculate losses for each head\n",
    "            estimate_loss = tf.keras.losses.SparseCategoricalCrossentropy()(\n",
    "                labels['estimate_number'], predictions['estimate_number'])\n",
    "            address_loss = tf.keras.losses.SparseCategoricalCrossentropy()(\n",
    "                labels['address'], predictions['address'])\n",
    "            total_loss = tf.keras.losses.MeanSquaredError()(\n",
    "                labels['total_amount'], predictions['total_amount'])\n",
    "            \n",
    "            # Combine losses\n",
    "            total_loss = estimate_loss + address_loss + total_loss\n",
    "        \n",
    "        # Compute gradients and update weights\n",
    "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss,\n",
    "            'estimate_loss': estimate_loss,\n",
    "            'address_loss': address_loss,\n",
    "            'total_loss': total_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPARATION PIPELINE\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "class DocumentDataPreprocessor:\n",
    "    def __init__(self, image_dir, labels_csv):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.labels_df = pd.read_csv(labels_csv)\n",
    "        \n",
    "        # Create vocabularies for text fields\n",
    "        self.estimate_tokenizer = self._create_estimate_tokenizer()\n",
    "        self.address_tokenizer = self._create_address_tokenizer()\n",
    "    \n",
    "    def _create_estimate_tokenizer(self):\n",
    "        # Create simple numeric tokenizer for estimate numbers\n",
    "        all_estimates = self.labels_df['estimate_number'].astype(str).tolist()\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=10000,  # Adjust based on your dataset\n",
    "            filters='',  # Keep all characters\n",
    "            lower=False,  # Preserve case\n",
    "            oov_token='<UNK>'\n",
    "        )\n",
    "        tokenizer.fit_on_texts(all_estimates)\n",
    "        return tokenizer\n",
    "    \n",
    "    def _create_address_tokenizer(self):\n",
    "        # Create tokenizer for addresses\n",
    "        all_addresses = self.labels_df['address'].astype(str).tolist()\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=10000,  # Adjust based on your dataset\n",
    "            filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "            lower=True,\n",
    "            oov_token='<UNK>'\n",
    "        )\n",
    "        tokenizer.fit_on_texts(all_addresses)\n",
    "        return tokenizer\n",
    "    \n",
    "    def preprocess_single_example(self, image_path, labels):\n",
    "        # Load and preprocess image\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.resize(image, [224, 224])\n",
    "        image = tf.keras.applications.imagenet_utils.preprocess_input(image)\n",
    "        \n",
    "        # Process labels\n",
    "        estimate_tokens = self.estimate_tokenizer.texts_to_sequences([str(labels['estimate_number'])])[0]\n",
    "        address_tokens = self.address_tokenizer.texts_to_sequences([str(labels['address'])])[0]\n",
    "        total_amount = float(labels['total_amount'])\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'labels': {\n",
    "                'estimate_number': estimate_tokens,\n",
    "                'address': address_tokens,\n",
    "                'total_amount': total_amount\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def create_dataset(self, batch_size=32):\n",
    "        # Create list of image paths and corresponding labels\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        for _, row in self.labels_df.iterrows():\n",
    "            image_path = self.image_dir / f\"{row['image_filename']}\"\n",
    "            if image_path.exists():\n",
    "                image_paths.append(str(image_path))\n",
    "                labels.append({\n",
    "                    'estimate_number': row['estimate_number'],\n",
    "                    'address': row['address'],\n",
    "                    'total_amount': row['total_amount']\n",
    "                })\n",
    "        \n",
    "        # Create tensorflow dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "        dataset = dataset.map(lambda x, y: self.preprocess_single_example(x, y))\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def save_vocabularies(self, output_dir):\n",
    "        # Save tokenizer vocabularies for later use\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save estimate number vocabulary\n",
    "        with open(output_dir / 'estimate_vocab.json', 'w') as f:\n",
    "            json.dump(self.estimate_tokenizer.word_index, f)\n",
    "        \n",
    "        # Save address vocabulary\n",
    "        with open(output_dir / 'address_vocab.json', 'w') as f:\n",
    "            json.dump(self.address_tokenizer.word_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTI-LINGUAL TEXT PROCESSING\n",
    "import spacy\n",
    "from transformers import MarianMTTokenizer, MarianMTModel\n",
    "import tensorflow as tf\n",
    "\n",
    "class MultilingualTextProcessor:\n",
    "    def __init__(self):\n",
    "        # Load Spanish and English language models\n",
    "        self.nlp_es = spacy.load('es_core_news_sm')\n",
    "        self.nlp_en = spacy.load('en_core_web_sm')\n",
    "        \n",
    "        # Load translation model\n",
    "        self.translator_es_en = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-es-en')\n",
    "        self.translator_tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-es-en')\n",
    "    \n",
    "    def detect_language(self, text):\n",
    "        # Simple language detection based on spaCy models\n",
    "        es_doc = self.nlp_es(text)\n",
    "        en_doc = self.nlp_en(text)\n",
    "        \n",
    "        # Compare confidence scores\n",
    "        es_score = sum(token.prob for token in es_doc)\n",
    "        en_score = sum(token.prob for token in en_doc)\n",
    "        \n",
    "        return 'es' if es_score > en_score else 'en'\n",
    "    \n",
    "    def translate_to_english(self, text):\n",
    "        # Translate Spanish text to English if needed\n",
    "        if self.detect_language(text) == 'es':\n",
    "            inputs = self.translator_tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "            translated = self.translator_es_en.generate(**inputs)\n",
    "            return self.translator_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        return text\n",
    "    \n",
    "    def process_description(self, text):\n",
    "        # Process mixed language description field\n",
    "        # First, detect main language\n",
    "        main_lang = self.detect_language(text)\n",
    "        \n",
    "        # Translate if Spanish\n",
    "        english_text = self.translate_to_english(text)\n",
    "        \n",
    "        # Extract key information (customize based on your needs)\n",
    "        doc = self.nlp_en(english_text)\n",
    "        \n",
    "        # Extract relevant entities\n",
    "        entities = {\n",
    "            'locations': [ent.text for ent in doc.ents if ent.label_ in ['LOC', 'GPE']],\n",
    "            'quantities': [ent.text for ent in doc.ents if ent.label_ == 'QUANTITY'],\n",
    "            'dates': [ent.text for ent in doc.ents if ent.label_ == 'DATE']\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'original_text': text,\n",
    "            'translated_text': english_text if main_lang == 'es' else None,\n",
    "            'main_language': main_lang,\n",
    "            'entities': entities,\n",
    "            'normalized_text': ' '.join([token.text for token in doc])\n",
    "        }\n",
    "    \n",
    "    def preprocess_multilingual_field(self, text, field_type):\n",
    "        \"\"\"\n",
    "        Preprocess specific fields based on their type\n",
    "        \"\"\"\n",
    "        if field_type == 'description':\n",
    "            return self.process_description(text)\n",
    "        elif field_type == 'address':\n",
    "            # Addresses usually don't need translation\n",
    "            return text.strip()\n",
    "        elif field_type == 'amount':\n",
    "            # Handle numeric values consistently\n",
    "            return text.replace('$', '').strip()\n",
    "        else:\n",
    "            return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTATION\n",
    "\n",
    "# Prepare data\n",
    "preprocessor = DocumentDataPreprocessor('path/to/images', 'labels.csv')\n",
    "train_dataset = preprocessor.create_dataset()\n",
    "\n",
    "# Initialize and train model\n",
    "model = DocumentExtractor(num_classes=YOUR_NUM_CLASSES)\n",
    "model.compile(optimizer='adam')\n",
    "model.fit(train_dataset, epochs=10)\n",
    "\n",
    "# Process mixed language content\n",
    "text_processor = MultilingualTextProcessor()\n",
    "processed_text = text_processor.process_description(description_text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
